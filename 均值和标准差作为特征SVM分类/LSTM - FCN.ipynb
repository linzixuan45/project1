{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " 参数:\n",
    "        input_size。输入的x中预期的特征数量\n",
    "        hidden_size。隐藏状态下的特征数量`h`。\n",
    "        num_layers。递归层的数量。例如，设置 \"num_layers=2 \"意味着将两个LSTM堆叠在一起。\n",
    "            意味着将两个LSTM堆叠在一起，形成一个 \"堆叠的LSTM\"。\n",
    "            第二个LSTM接收第一个LSTM的输出并计算最终结果。\n",
    "            计算最终结果。默认：1\n",
    "        bias: 如果`False`，那么该层不使用偏置权重`b_ih`和`b_hh`。\n",
    "            默认: `True```。\n",
    "        batch_first: 如果`True`，那么输入和输出张量将以`(batch, seq)`的形式提供。\n",
    "            以`(batch, seq, feature)`的形式提供，而不是`(seq, batch, feature)`。\n",
    "            请注意，这不适用于隐藏状态或单元状态。见下面的\n",
    "            输入/输出部分了解详情。 默认值: `False```。\n",
    "        dropout。如果非零，在每个LSTM层的输出上引入一个`Dropout'层，但最后一层除外。\n",
    "            LSTM层的输出，除了最后一层，放弃的概率等于\n",
    "            :attr:`dropout`。默认：0\n",
    "        bidirectional: 如果是`True'，成为一个双向LSTM。默认值：`False`。\n",
    "        proj_size: 如果``> 0``，将使用LSTM的相应大小的投影。默认值：0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_ITER = 5000\n",
    "BATCH_SIZE = 64\n",
    "DATA_PATH = './data/data.xls'\n",
    "NUM_CLASSES = 1\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 500\n",
    "SEQ_LENGTH = 3  # time step\n",
    "INPUT_SIZE = 3  # input size\n",
    "mm_x = MinMaxScaler()\n",
    "mm_y = MinMaxScaler()\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "def read_data(data_path):\n",
    "    data = pd.read_excel(data_path)\n",
    "    feature = data\n",
    "    label = data.iloc[:, [2]]\n",
    "    return feature, label\n",
    "\n",
    "\n",
    "# 标准化数据\n",
    "def normalization(x, y):\n",
    "    # print(x.values)\n",
    "    x = mm_x.fit_transform(x.values)\n",
    "    y = mm_y.fit_transform(y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# 建立滑动窗口\n",
    "def sliding_windows(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data) - SEQ_LENGTH - 1):\n",
    "        _x = data[i:i + SEQ_LENGTH, :]\n",
    "        _y = data[i + SEQ_LENGTH, -1]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# 建立DataLoader\n",
    "def data_generator(x_train, y_train, x_test, y_test):\n",
    "    train_dataset = TensorDataset(torch.from_numpy(x_train).to(torch.float32),\n",
    "                                  torch.from_numpy(y_train).to(torch.float32))\n",
    "    test_dataset = TensorDataset(torch.from_numpy(x_test).to(torch.float32), torch.from_numpy(y_test).to(torch.float32))\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False)\n",
    "    test_Loader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False)\n",
    "    return train_loader, test_Loader\n",
    "\n",
    "\n",
    "feature, label = read_data(DATA_PATH)\n",
    "\n",
    "feature, label = normalization(feature, label)\n",
    "\n",
    "x, y = sliding_windows(feature)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "train_loader, test_loader = data_generator(x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "# 建立 LSTM 模型\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_length = SEQ_LENGTH\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h_0 = torch.zeros(\n",
    "        #     self.num_layers,\n",
    "        #     BATCH_SIZE, self.hidden_size\n",
    "        # )\n",
    "        # c_0 = torch.zeros(\n",
    "        #     self.num_layers, BATCH_SIZE, self.hidden_size\n",
    "        # )\n",
    "        output, (h_n, c_n) = self.lstm(x, None)\n",
    "        h_out = output[:, -1, :]\n",
    "        # h_n.view(-1, self.hidden_size)\n",
    "        out = self.fc(h_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = LSTM(num_classes=NUM_CLASSES, input_size=INPUT_SIZE,\n",
    "             hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def train():\n",
    "    iter = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            batch_y = Variable(torch.reshape(batch_y, (len(batch_y), 1)))\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter += 1\n",
    "            if iter % 100 == 0:\n",
    "                print(\"iter: %d,    loss: %1.5f\" % (iter, loss.item()))\n",
    "\n",
    "\n",
    "def eval(test_x, test_y):\n",
    "    model.eval()\n",
    "    test_x = Variable(torch.from_numpy(test_x).to(torch.float32))\n",
    "    test_y = Variable(torch.from_numpy(test_y).to(torch.float32))\n",
    "    train_predict = model(test_x)\n",
    "    data_predict = train_predict.data.numpy()\n",
    "    y_data_plot = test_y.data.numpy()\n",
    "    y_data_plot = np.reshape(y_data_plot, (-1, 1))\n",
    "    data_predict = mm_y.inverse_transform(data_predict)\n",
    "    y_data_plot = mm_y.inverse_transform(y_data_plot)\n",
    "\n",
    "    plt.plot(y_data_plot)\n",
    "    plt.plot(data_predict)\n",
    "    plt.legend(('real', 'predict'), fontsize='15')\n",
    "    plt.show()\n",
    "\n",
    "    print('MAE/RMSE')\n",
    "    print(mean_absolute_error(y_data_plot, data_predict))\n",
    "    print(np.sqrt(mean_squared_error(y_data_plot, data_predict)))\n",
    "    print(y_data_plot.flatten()[:20])\n",
    "    print(data_predict.flatten()[:20])\n",
    "\n",
    "\n",
    "train()\n",
    "eval(x_test, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}